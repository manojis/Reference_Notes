hive --version
hadoop version
pig --version
sqoop version
oozie version

==========================================
hadoop fs -ls
hadoop fs -put purchases.txt
hadoop fs -tail purchases.txt
almost same as the unix commands

create a directory in hadoop with new folder name and put the purchases.txt in that folder.
hadoop fs -mkdir newFolder
hadoop fs -put purchases.txt newFolder - for inserting the data into hadoop folder
hadoop fs -ls newFolder  // shows the content of the file in the newFolder
hadoop fs -get {hadoop folder path} {localdisk folderpath\purchases.txt} - for retrieving the data from hadoop cluster

from Jupyter Notebook
=====================
!hdfs dfs -mkdir assignment1
!hdfs dfs -touchz  assignment1/test.txt
!hdfs dfs -put assignment1/test.txt
!hdfs dfs -ls -R -t
!hdfs dfs -chmod 700 assignment1/test.txt
!hdfs dfs -cat assignment1/test.txt | head -n 10
!hdfs dfs -mv assignment1/test.txt assignment1/test2.txt

!hdfs fsck /data/wiki/en_articles_part/articles-part/ -blocks -locations
!hdfs fsck /data/wiki/en_articles_part/articles-part/ -files -blocks 
&
!hdfs fsck -blockId blk_1073741825_1001

to get the live data nodes and details regarding the hadoop storage:-
!hdfs dfsadmin -report

=====================================
for creating a jar file - jar -cvf {jar name.jar} *.class

Hadoop commands:
===============
hadoop fs :-
=============
hadoop file system commands like CAT, CHMOD, CHOWN, ls, CHGROUP, GET,PUT etc

hadoop jar :-
============
Runs a jar file. Users can bundle their Map Reduce code in a jar file and execute it using this command.
Usage:-
======
hadoop jar <jar> [mainClass] args..
hadoop jar /usr/share/hue/apps/oozie/examples/lib/hadoop-examples.jar wordcount shakespeare wordcount-output
hadoop jar {jar file with path} {Main Driver class from the jar} {input directory} {output directory}
hadoop fs -cat output-workdcount/part-r-00000(this is the output file which tells only one reducer. if 6 reducer then 6 files)
hs {script} {reducer script} {input_file} {output directory}

hadoop does not allow you to override the data in the cluster.create timestamp for output filenames.

examples of mapreduce: Recommendation systems, fraud Detection,Item Classification etc


Objective:-
To have the daily web log files collected from 350+ servers daily queryable thru some SQL like language
To replace daily aggregation data generated thru MySQL with Hive
Build Custom reports thru queries in Hive

Architecture Options:-
I benchmarked the following options
Hive+HDFS
Hive+HBase - queries were too slow so dumped this option

Design:-
Daily log Files were transported to HDFS
MR jobs parsed these log files and output files in HDFS
Create Hive tables with partitions and locations pointing to HDFS locations
Create Hive query scripts (call it HQL if u like as diff from SQL) that in turn ran MR jobs in the background and generated aggregation data
Put all these steps into an Oozie workflow - scheduled with Daily Oozie Coordinator
Summary

HBase is like a Map. If u know the key, u can instantly get the value. But if u want to know how many integer keys in Hbase are between 1000000 and 2000000 that is not suitable for Hbase alone.

If you have data to be aggregated, rolled up, analyzed across rows then consider Hive

Pig:-
Pig is a simple-to-understand data flow language used in the analysis of large data sets. Pig scripts are automatically converted into MapReduce jobs by the Pig interpreter, so you can analyze the data in a Hadoop cluster even if you aren't familiar with MapReduce.


The original file is divided into smaller chunks and pushed into each node inside the HDFS. Each node containing the file is handled by the DataNode Daemon and the location of each file in the nodes is provided in the metadata(information stored in the NameNode) of the NameNode.
Hadoop replicates the data three times in different nodes to ensure the data is not lost.
Types of hadoop mode:-
=======================
Hadoop run modes and job Type:-
==============================
  local JobRunner:-
        Only single JVM, but mocks up all the daemons as if they are running on separate real jvms.it shares all the heap and all variables inside of a single jvm.
  Psuedo Distributed jobRunner:-
        Here all the daemons runs as single jvm's inside one single machine.Each Task tracker creates a new jvm when a task is scheduled by JT.It is started and shut down (after task completion).Each Task tracker handles multiple task and hence creates multiple jvm for handling that task which is quite an expensive process.
  Fully distributed Jobrunner mode
      Running on 1000s of servers for mapping and reducing

Network time protocol need to be synced in all the nodes

single distributed mode
psuedo-distributed mode - full cluster running on one machine.
https://docs.google.com/document/d/1v0zGBZ6EHap-Smsr3x3sGGpDW-54m82kDpPKC2M6uiY/pub
https://docs.google.com/document/d/1MZ_rNxJhR4HCU1qJ2-w7xlk2MTHVqa9lnl_uj-zRkzk/pub
Fully distributed mode
https://developer.yahoo.com/hadoop/tutorial/module5.html#partitioning

Hadoop Streaming which allows us to write the code in any language for mappers and reducers.
Hadoop job tracker webInterface--> localhost:50030/jobtracker.jsp

The data pushed into the HDFS file system is split into chunks and replicated default 3 times and distributes among slaves.This replication is done for durability and concurrent access.
Write once and Read many- NO appends or edits.
Blocksize in hadoop is 64MB but better to have 128MB.
for replication factor of 3, the same data will be replicated across three data nodes.

NameNode ->
=========
Namenode is the masternode the clients must initiate read/write operation.
Has all metadata info like filename, permissions,directory and which nodes contain which blocks.
Job Tracker- individual daemons which runs on each master nodes
contains two files inside the NameNode -> fsimage and Edit Logs
if we loose name node, then we lose HDFS.
NameNode contains fsimage and editlogs.

fsimage and edit logs are periodically merged.
fsimage - Its the snapshot of the filesystem when namenode is started and is created by the secondary name node and sits inside both namenode and the 2namenode(this new one is copied to namenode after every 1 hour)
Edit logs - Its the sequence of changes made to the filesystem after namenode is started or is the HDFS deltas since snapshot.
2NN uses this editlogs to do compaction and ensure the new fsimage is created based on the editlogs.It makes hadoop runs fast as the editlogs will be small.
RTO or recovery time objective will grow longer if 2NN is down for a longtime.
RPO - recovery point objective. i.e amount of data loss.
No hotfailover mechanism by secondary Name Node.
http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/
Atleast two mirrored RAIDed disks, a remote NFS mountpoint and heavy monitoring of all disks
Daily or weekly backups of the full fsimage and the edit logs.

DataNode ->
Slave Node where the individual block storage happens
client does direct read/write to/from slave nodes.
commodity server
Datanode also provides the block reports, providing info on the blocks it has at regular intervals.if one of the datanodes has lesser block than the replication, then the namenode will pull the block across the network from another data node which has it, to ensure the replication factor is maintained.

Task Tracker - Individual daemons which runs on each slave nodes.

HeartBeats:
Every 3 seconds from data node into name node that it is alive and ready to take task(i.e read/write)
if after 10 heartbeats or 30 seconds, it does not report to the name node, the datanode is taken out of rotation and that block is re-replicated.

Integrity is also ensured through checksums. i.e comparison of the write checksum when a write is made(by writing to a file in the same data node).This file is compared with read checksum during the reading by the client. if the checksum are not equal, then namenode is notified and the data is read from another data node and the bad datanode is re-replicated with data from another node.The data node also verifies the checksum integrity internally every three weeks and re-replication happens if its not proper.

***************** HDFS FILE WALKTHROUGH ********************
Go through the HDFS file walkthrough
hadoop fs -put {inputfile} {outputfolder}

provide the block size and replication factor and then client will divide the file into the chunks.
replication factor and the block size is provided by the client and mostly it is stored in the client locally. It can also be provided from the name node
total size/block size = number of splits

  # initially the client talks to the name node of the hadoop with a PUT request using hadoop fs command for a write operation with the blocksize and the replication factor and the first blocksize.
  # the namenode responds with "WRITE PIPELINE" provides with a set of the data nodes where it needs to write to based on the replication factor.
  "DATE NEVER MOVES THROUGH THE NAMENODE"
  # client connects with the firstdata node and then puts the file into the first datanode's local disk and from there it gets copied internally to other nodes like client -> A -> c ->F for a replication factor of 3.
  # A Reverse acknowledgement happens like F -> C -> A -> client
  # As soon as the client gets notified about a successful write in all nodes, it updates the namenode about the success and provides info about the locations of the first block and ask for the second block.
  # The namenode updates its memory and disk representation with this data with one block long and then next set of iteration happens.
  # after all the blocks are done, the file is immutable.
  # the same mapping is there in namenode's memory and disk.
  # all data nodes are equal.
  # Incase some of the nodes as part of the replication is down, the namenode and datanode will ensure the replication based on the success from the updated data node.Incase if all are down, the client will reach out to the namenode for a new write pipeline.

Checksum:-
checksum also does its job here to ensure integrity
======================================================================================
HA(High availability and federation basics):-
-------------------------------------------
HA- to prevent namenode being the single point of failure.
standby name node came into picture.
https://www.safaribooksonline.com/library/view/learning-apache-hadoop/9781771372374/video183091.html
https://www.safaribooksonline.com/library/view/learning-apache-hadoop/9781771372374/video183092.html
kerberos provides strong authentication mechanism to HDFS, which is done through ticket granting service.

======================================================================================
ports:-
NameNode 50070
Job Tracker 50030
Task Tracker 50060

Design Patterns in Hadoop:-
1). Filtering patterns -
      Sampling , Top-N No Change in the original data- output is created
      Simple filter
      Bloom Filter
      Sampling Filter
      Random Filter
      Top-N Filter
2). Summarization patterns -
      Counting records, Max/Min, Statistics,Index
Structural patterns - Combining Data sets


MAPPER: Data Extractor
REDUCER: Data Summarization

Master and Slave Node configurations:
=====================================
Master Nodes would be like 2-3 node clusters.
These are single point of failures
They require redundancy with carrier class hardware.

Slave Nodes:
4-4000 node cluster
commodity class cluster
Hadoop handles redudancy internally.
Slave Nodes are added based on storage or processing needs.

Hadoop Uses Linear horizontal scale.
Cluster Growth:
if the storage is a factor for growth for slave nodes ->Raw data-intermediate Date(which is 20 to 30% of Raw)/R.f = usable disk space
if the processing is a factor for growth for slave nodes -> Need to increase the cpu or ram

Hadoop Ecosystem:-
=================
anything outside of hdfs or mapreduce
ZooKeeper is used for building the distributed systems.
Yarn - yet another resource negotiator
Hive -> sql like interface - mapreduce abstractions
pig -> provide functional programming interface-mapreduce abstractions
HBase -> Fast scalable NOSQL engine
Cloudera Managers -> Web-Interface for administrators
Impala - real time sql queries -> makes sql a first class citizen

Hadoop architecture is batch processing architecture based on parallelism, i.e data should be pushed in multiple nodes and processable.

Types of analysis;-
Recommendation engine
Index building
Text mining/pattern recognition
Risk assessment/threat analysis
predictive analysis
fault or error analysis

five long running daemons in the hadoop cluster
Storage:
NameNode -> Master Daemons
SecondaryNameNode -> Slave Daemons -> performs "checkpointing" only for namenodes
DataNode -> Slave Daemons

processing:-
job Tracker -> Master Daemons-
coordinates the processing of namenode
coordinates the scheduling of job processing.
Task Tracker->Slave Daemons-> processing task are handled by a long running daemons, which handles individual piece of assignment.
Slave nodes provide health output and data status everytime to jobnode

Hive:-
run the querying by converting it into MR.
Hive converts its tables into HDFS
Requires pre-creation and population of tables.
works better for ad-hoc queries.Works better with structured data.

Pig:
Requires no metastore and works on raw HDFS files
load and define the data type at the runtime.
get the data from HDFS file store, for which we need to run the query and use that file to create and run the query.

find . -type d| grep hue

In new version of hadooop, jobs can be monitored at:
localhost:8088

In old version,hadoop:-
NameNode - (http://localhost:50070)/ (for my name node)
JobTracker - (http://localhost:50030)/

To track the job tracker or the ResourceManager and applicationMaster:-
=====================================================================
If you are using YARN framework, there is no jobtracker in it. Its functionality is split and replaced by ResourceManager and ApplicationMaster.
Here is expected jps printout while running YARN
/usr/lib/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh

Hadoop 2 uses YARN framework i.e. MapReduce 2.0 (MRv2).start-mapred.sh is not required. To start Hadoop, use start-all.sh or start-dfs.sh and start-yarn.sh.
YARNâ€™s original purpose was to split up the two major responsibilities of the JobTracker/TaskTracker into separate entities:
    # global ResourceManager
    # per-application ApplicationMaster
    # per-node slave NodeManager
    # per-application Container running on a NodeManager

Check the job.xml from the ResourceManager's UI by navigating to http://RESOURCEMANAGER_HOST:8088/cluster -> select your application's Tracking UI -> Select your Job ID -> On the left tab you'll be able to see Configuration. Or simply if you already know you'r job's id then visit this link: http://JOBHISTORY_SERVER:19888/jobhistory/conf/YOUR_JOB_ID
============
MAPREDUCE:-
============
The output from the mappers are the Intermediate Records.All the forms of data are key/value pair
Once the mapping completes, shuffle and sort happens.
Hadoop takes care of the Shuffle and Sort phase. You do not have to sort the keys in your reducer code, you get them in already sorted order.
The shuffle is the movement of the key/value pair from the intermediate records to the Reducers and the sort is the sorting of these data sets in the reducer.
Thus reducers gets a key and then a list of all the values of the key like for example all the values of the different amount from each store.

When we submit a job to a mapreduce, we submit the job to JobTrackers. That splits the work into mappers and reducers.these mappers and reducers run on other nodes and each of these task is handled by a tasktracker and run on each of these nodes.

Each node will have multiple input split which needs to completed as task and it is the task tracker of the same node which handles it and incase if any of the tasktracker is already involved in handling one of the splits of the same node, then a task tracker from another node will handle the tracking of the job, which happens rarely.

1 is the least number of mappers required for a job in mapreduce and zero is the least number of reducers required for Mapreduce,as aggregation/summation may not be required always.

Number of mappers required for processing the job is handled by the hadoop itself.
Number of reducers required for processing the job is handled by the users.

Job Tracker(Master Daemon):-
--------------
client submits the job to job tracker.Job tracker is constantly pinged by the task trackers.
It does the orchestration and delegation.

Task Tracker:
-------------
slave daemon.
Task tracker always ping saying that it is ready to accept the data for map and reduce task.
task tracker in each node always tracks the progress of its map or reduce tasks.
Task tracker always reports the progress of its map and reduce tasks to the job tracker on a regular interval with which we can see in the terminal, the completed % of the whole map or reduce tasks.
Mapper inside the task tracker gets the data inside the same node (or another node in extreme case), and completes the task and outputs an intermediate output.The task tracker provides the update of the progress to the jobtracker, which initiates another data node to start the reducer task. This reducer task gets the intermediate output from each of the mapper data node across the network for the next processing.
once all the processing in reducers is complete, the reducers act as the client and output the data into the HDFS.

MapReduce config files(location: /etc/hadoop/conf):-  mapred-sit.xml-
core mapreduce daemon config
hadoop-env.sh -
  Environment variables used by the hadoop like jvm heap size, java_home etc

"Hadoop jar" tells the client to submit the job to job tracker with location. This is configured in mapred-site.xml
jarfile contains the jobconfig and all map and reduce code.

Speculative execution: - when a particular task runs slower than the other ones, job tracker assigns that task to another task tracker in another node as well and which ever output first, the other one would be removed.
Once all the job has been completed successfully, a _SUCCESS flag file is created in the output directory and Job tracker ask all the task tracker to remove the intermediate data from its locations.
log files still persist
command:
---------
hadoop jar {jar file with path} {Main Driver class from the jar} {input directory} {output directory}
Once the job is submitted into the cluster, we cannot kill the job by the cntrl+c.
To kill the job, we need to run the hadoop dash kill command.

Driver code: It controls all the configuration.our job level parameters.Types of input or output.
mapper code
Reducer code- simple aggregates the output.
========

javac - classpath 'hadoop classpath' *.java
jar cvf wordcount,jar *.class
========
The client has been configured with location of the job tracker in the mapred-site.xml file

MapReduce configuration files:-
================================
MapReduce is one type of application that can run on the Hadoop 2.x framework. MapReduce configuration options are stored in the /opt/mapr/hadoop/hadoop-2.x.x/etc/hadoop/mapred-site.xml file and are editable by the root user. This file contains configuration information that overrides the default values for MapReduce parameters. Overrides of the default values for core configuration properties are stored in the Default mapred Parameters file.

mapreduce methods:
inside Driver:
create an instance of job
setDriverClass()
setMapperClass()
setReducerClass()

output needs to be in key value :-
setOutputKeyclass(Text.class)
setOutputValueClass(IntWritable.class)

fileinputformat.addfilepath()
fileoutputformat.addfiletpath()

mapper.class:-
extends mapper class
map method



To override a default value for a property, specify the new value within the <configuration> tags, using the following format:
<property>
 <name> </name>
 <value> </value>
 <description> </description>
</property>


RackAwareness:-
================
Embed the racknumber in our hostnames or provide a script in the name node with all the datanode rack details like ip and server name, rack name etc(not good).
like S123-r1 in
core-site.xml ->topology.script.file.name parameter
This will ensure namenode becomes rack aware.

CUSTOM COMPARATOR, COMBINER AND PARTITIONERS:
=============================================
For sort functions, we need to implement custom comparator function needs to be implemented.
WriteableComparable must implement "equals","compareTo" and "hashCode" methods.
calling job.setSortComparatorClass(MyComparator.class) in driver for comparator.class

All jobs needs to be mapped or partitioned based on keys.
partitioner methods can be hashpartitioner based on hashcode or TotalorderPartitioner or Custom partitioner.
job.setpartitioner in driver by passing the name of the custom partitioner class we created ---> job.setpartitionerClass(mycustomParitioner.class)
--> partitioners,combiners and comparators and custom writablecomparables have to be written in java



Datatypes in hadoop:-
-------------------
Text
IntWritable
LongWritable
FloatWritable
 etc
 all are WritableComparables.(dint read on this)

 Debugging:
 ----------
 Tough in fully distributed mode.
 ensure the datatypes match with the actual data and take care of the malformed data through try and catch.
 use IDE with local job runner mode.
 use MRUnit framework for unit testing of the mappers and reducers.

Logs:
no print statement
Job Level - Output provides the data inside HDFS-Output-dir/_logs(This also contains the xml configuration submitted by "Hadoop jar" command)
Task Tracker - inside /var/log/hadoop-version/userlogs/{}/syslog

apache log4j logger used by hadoop
private static final logger LOGGER = LOGGER.getLogger(MyMapper.class.getName());
log4j log level config location :- /etc/hadoop/conf/log4j.properties

HIVE-PIG-IMPALA:
------------------
Hive and pig are alternative interfaces to mapreduce
Hive and Impala are good for analytics

Pig for processing or ETL(Extract,Load and Transform).Used for complex data flow. Requires a pig interpreter.Uses the piglatin scripts and converts them into mapreduce jobs and run it on HDFS to provide the output.
Both pig and Hive can be installed as a client or as a Webservice(Hive2)

IMPALA:
data maps well into row/column format.
and needed real time interactive querying capability.
Need to ensure lack of fault tolerance is acceptable.
has IMPALAD daemon which runs on the slavenodes.no need of job tracker and task tracker and impala queries works fine as these daemons
has statestored daemon. run 1 on each cluster.

HIVE:-
======
HIVE used when the data maps to row/column format.or data analyst are SQL pros.
Hive can be installed as a client or as a webservice(Hive2 or Hue)
HiveServer2 is used for connection with application using jdbc connections.
Requires metastore for either client or server which is the mapping between Hive table and HDFS data.
metastore does not store any data in itself.just a relationship mapper with a hdfs data.
eg: Create External table page_view(ViewTime int, userId BigInt, page_url String, referre_url String, IP string)
    COMMENT 'this is a page view table' ROW FORMAT DELIMITED FIELD TERMINATED BY '\t' LOCATION '{some path in HDFS}';
#data in hdfs
213124^t1231^thttp://asfda.com/^thttp://sfadc.com/^t182.232.32.1
Now in Hive i can run a hive query like:-
Select * from page_view where page_url like "%asfda.com";

Internal vs External - Internal table will affect the HDFS data as well along with the schema in metastore, but not the External table.

HDFS is a fault tolerant distributed storage on large clusters of commodity hardware.
hdfs-default.xml
JobTracker and TaskTracker are 2 essential process involved in MapReduce execution in MRv1 (or Hadoop version 1). Both processes are now deprecated in MRv2 (or Hadoop version 2) and replaced by Resource Manager, Application Master and Node Manager Daemons.
http://hadoopinrealworld.com/jobtracker-and-tasktracker/

Objective:-
To have the daily web log files collected from 350+ servers daily queryable thru some SQL like language
To replace daily aggregation data generated thru MySQL with Hive
Build Custom reports thru queries in Hive

Architecture Options:-
I benchmarked the following options
Hive+HDFS
Hive+HBase - queries were too slow so dumped this option

Design:-
Daily log Files were transported to HDFS
MR jobs parsed these log files and output files in HDFS
Create Hive tables with partitions and locations pointing to HDFS locations
Create Hive query scripts (call it HQL if u like as diff from SQL) that in turn ran MR jobs in the background and generated aggregation data
Put all these steps into an Oozie workflow - scheduled with Daily Oozie Coordinator
Summary

HBase is like a Map. If u know the key, u can instantly get the value. But if u want to know how many integer keys in Hbase are between 1000000 and 2000000 that is not suitable for Hbase alone.

If you have data to be aggregated, rolled up, analyzed across rows then consider Hive

Hbase:
CRUD (create, read, update, and delete) data. You can also perform scans of HBase table rows, which are always stored in HBase tables in ascending sort order. When you scan through HBase tables, rows are always returned in order by row key. Each row consists of a unique, sorted row key (think primary key in RDBMS terms) and an arbitrary number of columns, each column residing in a column family and having one or more versioned values.


Cloudera Manager:-
================
Event server
Host monitor
Alert publisher
service monitor

Container in yarn:-
================
It represents a resource (memory) on a single node at a given cluster.
A container is:-
supervised by the node manager
scheduled by the resource manager
One MR task runs in such container(s).

MapReduce v1 vs MapReduce v2:-
===========================
YARN dynamically allocates resources for applications as they execute. The MapReduce version 1 (MRv1) has been rewritten to run as an application on top of YARN; this new version is called MapReduce version 2.0 (MRv2).The main advancement in YARN architecture is the separation of resource management and job management, which were both handled by the same process (the JobTracker) in Hadoop 1.x. Cluster resources and job scheduling are managed by the ResourceManager, and resource negotiation and job monitoring are managed by an ApplicationMaster for each application running on the cluster. In MapReduce, each node advertises a relatively fixed number of map slots and reduce slots. This can lead to resource under-utilization, for example, when there is a heavy reduce load and map slots are available, because the map slots cannot accept reduce tasks (and vice versa).
YARN generalizes resource management for use by new engines and frameworks, allowing resources to be allocated and reallocated for different concurrent applications sharing a cluster. Existing MapReduce applications can run on YARN without any changes. At the same time, because MapReduce is now merely another application on YARN, MapReduce is free to evolve independently of the resource management infrastructure.

hadoop jar {path to hadoop jar} {main class} input-dir {directory name} output-dir
Driver controls all the configuration and this is where we setup our job level parameters.

Config files:
===========
Each client, masterNode and slave Node has MR Config files -
/etc/hadoop/conf -
mapred-site.xml - has core MapReduce Daemon config
hadoop-env.sh - all jvm, memory environments variables.


The various job control options are:
Job.submit() : to submit the job to the cluster and immediately return
Job.waitforCompletion(boolean) : to submit the job to the cluster and wait for its completion

inputSplit:-
Map reads data from Block through splits i.e. split act as a broker between Block and Mapper.

MapReduce v1:-
client
Job Tracker Daemon - Master Daemons
Task Tracker Daemon - Slave Daemons
Map and Reduce short Lived Daemons

The four parameters for mappers are:
LongWritable (input)
text (input)
text (intermediate output)
IntWritable (intermediate output)
The four parameters for reducers are:

Text (intermediate output)
IntWritable (intermediate output)
Text (final output)
IntWritable (final output)


Reducer has 3 primary phases: shuffle, sort and reduce.
The output of the reduce task is typically written to the FileSystem via OutputCollector.collect(WritableComparable, Writable)

zookeeper-
HDFS
HBase - Database - Fast scalable NoSQL engine
solr
yarn
sqoop 2 - pull/push data to RDBMS
key value store indexer
spark -
hive - MR - provide sql like interface
pig - MR - provide functional programming interface
oozie - chain together MR jobs, data import/export scripts
impala - makes a sql first class citizen
Hue - web interface for end users
Flume - push real time data to HDFS

sudo ~/cloudera-manager --force

SequenceFile:-
is a flat file consisting of binary key/value pairs. It is extensively used in MapReduce as input/output formats. It is also worth noting that, internally, the temporary outputs of maps are stored using SequenceFile.The SequenceFile provides a Writer, Reader and Sorter classes for writing, reading and sorting respectively.
There are 3 different SequenceFile formats:
Uncompressed key/value records.
Record compressed key/value records - only 'values' are compressed here.
Block compressed key/value records - both keys and values are collected in 'blocks' separately and compressed. The size of the 'block' is configurable.

Apache Hive:-
============
Hive is sql layer translation on top of hadoop mapreduce and not a pure RDBMS
Sql like interface to the data stored in hadoop - sql like syntax.
we are converting the sql query to a mapreduce in hadoop.
provides JDBC connectivity
sql to mapreduce is handled by the hive using catalog of information stored in the cluster - hive metastore or hcatalog.
hadoop deployments CDH,HDP and MapR

To run a query from a batch file:-
hive -e "show tables" instead of going into Hive cli and running the command
"hive -f testfile.sql" will run the testfile.sql file containing the hive query.

b
hive --service hiveserver2 --hiveconf hive.server2.thrift.port=12345

DriverManager.getConnection("");
Create a table stored as text
Insert the text file into the text table
Do a CTAS to create the table stored as a sequence file.
Drop the text table if desired
Example:
1). CREATE TABLE test_txt(field1 int, field2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' stored as textfile;
2). LOAD DATA INPATH '/path/to/file.tsv';
3). CREATE TABLE test STORED AS SEQUENCEFILE AS SELECT * FROM test_txt;
4). DROP TABLE test_txt;

HiveServer2:- Beeline is the cli for HiveServer2 and is a JDBC client
Supports restAPI
Kerberos Authentication
authorization
multi-client concurrency
HiveServer2 Thrift API spec
JDBC/ODBC HiveServer2 drivers

Comparing Beeline to the Hive CLI:-
==================================
The primary difference between the two involves how the clients connect to Hive.
The Hive CLI, which connects directly to HDFS and the Hive Metastore, and can be used only on a host with access to those services.
Beeline, which connects to HiveServer2 and requires access to only one .jar file: hive-jdbc-<version>-standalone.jar.Hortonworks recommends using HiveServer2 and a JDBC client (such as Beeline) as the primary way to access Hive. This approach uses SQL standard-based authorization or Ranger-based authorization. However, some users may wish to access Hive data from other applications, such as Pig. For these use cases, use the Hive CLI and storage-based authorization.

Beeline modes of operation:
Embedded(Both Beeline and hive on the same machine) and Remote(different machines.Helps in multiple concurrent clients executing queries against the same remote hive installation.supports authentication with LDAP and Kerberos).

Authentication: hive.server2.authentication=kerberos or LDAP in the hive-site.xml

Hiveserver2 Modes of operation:
TCP and HTTP
Stope the Hive server, Hcatalog and metastore daemon on each client
sudo service hiveserver2 stop
sudo service hive-webhcat-server stop
sudo service hive-metastore stop

hadoop fs -put {source path to text file which needs to be inserted} {destination path to text file where it needs to be present}
hadoop fs -ls {destination path in hadoop eg. /user/hive/warehouse/myfirsttable}

when an external table is dropped, it actually deletes the metadata only and not the entire data in hdfs
Hive uses a relational database as its metastore/Hcatalog for tracking the information and hence it should never be updated manually.
LOAD DATA LOCAL INPATH {hadoop path in hive for input file} INTO TABLE {tablename};
describe extended {tablename};
describe formatted {tablename};
Hive Record Structure(SEQ-flat file binary key/value, AVRO-Json format,ORC-optimized row column format(brings in indexing)):-
=======================
default record format is text file or collection of text file, with a default delimiter which ^A, row format
Create table t1_Seq(Id int, type String) Stored as SEQ;
Create table t2_avro(Id int, type String) Stored as AVRO;
Create table t2_orc(Id int, type String) Stored as ORC;
Create table t2_parquet(Id int, type String) Stored as PARQUET;

to write the output to a file:-
-->Insert overwrite direcoty '/user/cloudera/hive_out' select * from euTable where year>1973;
Hive supports view but it does not support materialized views like the RDBMS where the view updates the rows when actual table updates.NO Directory would be created for the view.

data is stored in column format in AVRO,ORC,PARQUET for better data compression instead of row format.
To have an ORC format, we need to use Tez instead of MAP-REDUCE
datatypes in hive - INT, STRING, Binary,VARCHAR,char etc + map<int,String>,array<String>,struct<name:String,age:int>

How Hive Reads data in Records or columns:-
Input HDFS file --> INPUTFORMAT --> Deserializer(SerDe) --> Record
How Hive writes data from Record or column to HDFS file:-
Record --> Serializer(SerDe) --> OUTPUTFORMAT --> HDFS File

Partitioning and bucketing a table in HIVE:-
===========================================
partition helps in distributing the load horizontally.Partitioning tables changes how Hive structures the data storage and Hive will now create subdirectories reflecting the partitioning structure.Having too many partitions is the large number of Hadoop files and directories that are created unnecessarily and overhead to NameNode since it must keep all metadata for the file system in memory.
Bucketing is another technique for decomposing data sets into more manageable parts.The column which has to be in bucket(should be the secondary indexing one), then that particular column Id's hashing will be taken and put into each bucket for faster querying just like hashset. CLUSTERED BY (employee_id) INTO XX  BUCKETS; where XX is the number of buckets.
Difference is bucketing divides the files by Column Name, and partitioning divides the files under By a particular value inside table
Ex:
CREATE TABLE mytable (name string, city string, employee_id int) PARTITIONED BY (year STRING, month STRING, day STRING) CLUSTERED BY (employee_id) INTO 256 BUCKETS;
hive will store data in a directory hierarchy like: /user/hive/warehouse/mytable/y=2015/m=12/d=02

Hue:
===
192.168.1.101:8888
query editor - beewax

Hbase:
stop the thrift server and clients followed by the region servers and finally the master.
sudo service hbase-thrift stop
sudo service hbase-rest stop
sudo service hbase-regionserver stop
sudo service hbase-master stop
